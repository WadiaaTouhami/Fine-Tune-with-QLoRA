# Project Overview
In this project, I fine-tuned the BLOOM-1B1 model for a news summarization task using the CNN-DailyMail News Text Summarization dataset from Kaggle. The fine-tuning process focuses on improving the model's ability to summarize large news articles into concise and coherent summaries.

# Key Features
- Model Architecture: BLOOM-1B1 (1.1 billion parameters) - a large language model.
- Dataset: CNN-DailyMail News Text Summarization dataset from Kaggle.
- Optimization Techniques:
    - Quantization: To reduce memory usage and inference time without significant performance loss.
    - LoRA (Low-Rank Adaptation): To efficiently fine-tune the model with minimal additional parameters.
